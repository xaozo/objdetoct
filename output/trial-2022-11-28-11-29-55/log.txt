[11/28 11:29:55] detectron2 INFO: Rank of current process: 0. World size: 1
[11/28 11:29:56] detectron2 INFO: Environment info:
----------------------  -----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]
numpy                   1.23.4
detectron2              0.6 @/home/general/detectron2/detectron2
Compiler                GCC 9.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          526.98
CUDA_HOME               /usr
Pillow                  9.3.0
torchvision             0.14.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     4.6.0
----------------------  -----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 11:29:56] detectron2 INFO: Command line arguments: Namespace(config_file='./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[11/28 11:29:56] detectron2 INFO: Contents of args.config_file=./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m../Base-RCNN-FPN.yaml[39m[38;5;186m"[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(210000,[39m[38;5;141m [39m[38;5;141m250000)[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m270000[39m

[11/28 11:29:56] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mval[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mtrain[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBGR[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m672[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m704[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m736[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_fpn_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGeneralizedRCNN[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_FREQ_WEIGHT_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_NUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFastRCNNConvFCHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_FED_LOSS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_SIGMOID_CE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_HEADS_BATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSemSegFPNHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m54[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output/trial-2022-11-28-11-29-55[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBASE_LR_END[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mvalue[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m320[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mNUM_DECAYS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mRESCALE_INTERVAL[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[11/28 11:29:56] detectron2 INFO: Full config saved to ./output/trial-2022-11-28-11-29-55/config.yaml
[11/28 11:29:56] d2.utils.env INFO: Using a generated random seed 56707355
[11/28 11:29:58] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=4, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)
    )
  )
)
[11/28 11:29:58] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:29:58] d2.data.datasets.coco INFO: Loaded 565 images in COCO format from ./dataset/train/labels_train.json
[11/28 11:29:58] d2.data.build INFO: Removed 0 images with no usable annotations. 565 images left.
[11/28 11:29:58] d2.data.build INFO: Distribution of instances among all 3 categories:
|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   break    | 503          |    disc    | 560          |   shadow   | 606          |
|            |              |            |              |            |              |
|   total    | 1669         |            |              |            |              |
[11/28 11:29:58] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[11/28 11:29:58] d2.data.build INFO: Using training sampler TrainingSampler
[11/28 11:29:58] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:29:58] d2.data.common INFO: Serializing 565 elements to byte tensors and concatenating them all ...
[11/28 11:29:58] d2.data.common INFO: Serialized dataset takes 0.21 MiB
[11/28 11:29:58] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[11/28 11:29:58] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:29:58] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:29:58] d2.data.build INFO: Distribution of instances among all 3 categories:
|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   break    | 212          |    disc    | 181          |   shadow   | 190          |
|            |              |            |              |            |              |
|   total    | 583          |            |              |            |              |
[11/28 11:29:58] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:29:58] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:29:58] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:29:58] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[11/28 11:29:58] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[11/28 11:29:58] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model    | Names in Checkpoint      | Shapes                                          |
|:------------------|:-------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |
[11/28 11:29:58] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
backbone.fpn_lateral2.{bias, weight}
backbone.fpn_lateral3.{bias, weight}
backbone.fpn_lateral4.{bias, weight}
backbone.fpn_lateral5.{bias, weight}
backbone.fpn_output2.{bias, weight}
backbone.fpn_output3.{bias, weight}
backbone.fpn_output4.{bias, weight}
backbone.fpn_output5.{bias, weight}
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_head.fc1.{bias, weight}
roi_heads.box_head.fc2.{bias, weight}
roi_heads.box_predictor.bbox_pred.{bias, weight}
roi_heads.box_predictor.cls_score.{bias, weight}
[11/28 11:29:58] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  fc1000.{bias, weight}
  stem.conv1.bias
[11/28 11:29:58] d2.engine.train_loop INFO: Starting training from iteration 0
[11/28 11:30:14] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:30:14] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:30:14] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:30:14] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:30:14] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:30:14] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:30:14] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:30:14] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:30:14] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0321 s/iter. Eval: 0.0004 s/iter. Total: 0.0331 s/iter. ETA=0:00:05
[11/28 11:30:19] d2.evaluation.evaluator INFO: Inference done 161/184. Dataloading: 0.0009 s/iter. Inference: 0.0320 s/iter. Eval: 0.0004 s/iter. Total: 0.0333 s/iter. ETA=0:00:00
[11/28 11:30:20] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.964821 (0.033323 s / iter per device, on 1 devices)
[11/28 11:30:20] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031865 s / iter per device, on 1 devices)
[11/28 11:30:20] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:30:20] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:30:20] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:30:20] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:30:21] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.07 seconds.
[11/28 11:30:21] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:30:21] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 11:30:21] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.003 | 0.016  | 0.001  | 0.000 | 0.000 | 0.099 |
[11/28 11:30:21] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.008 | shadow     | 0.000 |
[11/28 11:30:21] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:30:21] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:30:21] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:30:21] d2.evaluation.testing INFO: copypaste: 0.0027,0.0159,0.0006,0.0000,0.0000,0.0989
[11/28 11:30:32] d2.utils.events INFO:  eta: 0:04:11  iter: 19  total_loss: 2.453  loss_cls: 0.9601  loss_box_reg: 0.01047  loss_rpn_cls: 0.6823  loss_rpn_loc: 0.8735  validation_loss: 2.546  time: 0.8455  data_time: 0.0660  lr: 6.0316e-06  max_mem: 9915M
[11/28 11:30:43] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:30:43] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:30:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:30:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:30:43] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:30:43] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:30:43] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:30:43] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:30:43] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0316 s/iter. Eval: 0.0004 s/iter. Total: 0.0326 s/iter. ETA=0:00:05
[11/28 11:30:48] d2.evaluation.evaluator INFO: Inference done 158/184. Dataloading: 0.0009 s/iter. Inference: 0.0326 s/iter. Eval: 0.0005 s/iter. Total: 0.0340 s/iter. ETA=0:00:00
[11/28 11:30:49] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.083413 (0.033986 s / iter per device, on 1 devices)
[11/28 11:30:49] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032456 s / iter per device, on 1 devices)
[11/28 11:30:49] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:30:49] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:30:49] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:30:49] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:30:49] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.06 seconds.
[11/28 11:30:49] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:30:49] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 11:30:49] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.034 | 0.218  | 0.003  | 0.000 | 0.001 | 0.061 |
[11/28 11:30:49] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.101 | shadow     | 0.001 |
[11/28 11:30:49] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:30:49] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:30:49] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:30:49] d2.evaluation.testing INFO: copypaste: 0.0341,0.2175,0.0031,0.0000,0.0014,0.0609
[11/28 11:31:04] d2.utils.events INFO:  eta: 0:03:55  iter: 39  total_loss: 2.009  loss_cls: 0.3562  loss_box_reg: 0.01328  loss_rpn_cls: 0.6782  loss_rpn_loc: 0.8812  validation_loss: 2.296  time: 0.8463  data_time: 0.0529  lr: 1.2275e-05  max_mem: 9915M
[11/28 11:31:11] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:31:11] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:31:11] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:31:11] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:31:11] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:31:11] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:31:11] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:31:11] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:31:12] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0317 s/iter. Eval: 0.0004 s/iter. Total: 0.0328 s/iter. ETA=0:00:05
[11/28 11:31:17] d2.evaluation.evaluator INFO: Inference done 159/184. Dataloading: 0.0009 s/iter. Inference: 0.0315 s/iter. Eval: 0.0013 s/iter. Total: 0.0338 s/iter. ETA=0:00:00
[11/28 11:31:18] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.044957 (0.033771 s / iter per device, on 1 devices)
[11/28 11:31:18] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031489 s / iter per device, on 1 devices)
[11/28 11:31:18] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:31:18] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:31:18] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:31:18] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:31:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.08 seconds.
[11/28 11:31:18] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:31:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 11:31:18] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.010 | 0.069  | 0.001  | 0.000 | 0.000 | 0.016 |
[11/28 11:31:18] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.030 | shadow     | 0.001 |
[11/28 11:31:18] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:31:18] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:31:18] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:31:18] d2.evaluation.testing INFO: copypaste: 0.0103,0.0691,0.0006,0.0000,0.0003,0.0160
[11/28 11:31:37] d2.utils.events INFO:  eta: 0:03:40  iter: 59  total_loss: 1.637  loss_cls: 0.149  loss_box_reg: 0.01372  loss_rpn_cls: 0.6662  loss_rpn_loc: 0.7857  validation_loss: 2.046  time: 0.8519  data_time: 0.0533  lr: 1.8519e-05  max_mem: 9915M
[11/28 11:31:40] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:31:40] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:31:40] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:31:40] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:31:40] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:31:40] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:31:40] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:31:40] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:31:41] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0314 s/iter. Eval: 0.0004 s/iter. Total: 0.0324 s/iter. ETA=0:00:05
[11/28 11:31:46] d2.evaluation.evaluator INFO: Inference done 160/184. Dataloading: 0.0009 s/iter. Inference: 0.0322 s/iter. Eval: 0.0005 s/iter. Total: 0.0336 s/iter. ETA=0:00:00
[11/28 11:31:47] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.143574 (0.034322 s / iter per device, on 1 devices)
[11/28 11:31:47] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032072 s / iter per device, on 1 devices)
[11/28 11:31:47] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:31:47] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:31:47] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:31:47] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:31:47] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.07 seconds.
[11/28 11:31:47] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:31:47] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 11:31:47] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.004 | 0.024  | 0.000  | 0.000 | 0.001 | 0.006 |
[11/28 11:31:47] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.010 | shadow     | 0.002 |
[11/28 11:31:47] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:31:47] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:31:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:31:47] d2.evaluation.testing INFO: copypaste: 0.0038,0.0244,0.0002,0.0000,0.0006,0.0062
[11/28 11:32:10] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:32:10] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:32:10] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:32:10] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:32:10] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:32:10] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:32:10] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:32:10] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:32:10] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0326 s/iter. Eval: 0.0005 s/iter. Total: 0.0338 s/iter. ETA=0:00:05
[11/28 11:32:15] d2.evaluation.evaluator INFO: Inference done 159/184. Dataloading: 0.0009 s/iter. Inference: 0.0325 s/iter. Eval: 0.0005 s/iter. Total: 0.0339 s/iter. ETA=0:00:00
[11/28 11:32:16] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.061345 (0.033862 s / iter per device, on 1 devices)
[11/28 11:32:16] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032303 s / iter per device, on 1 devices)
[11/28 11:32:16] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:32:16] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:32:16] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:32:16] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:32:17] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.08 seconds.
[11/28 11:32:17] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:32:17] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 11:32:17] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.001 | 0.008  | 0.000  | 0.000 | 0.000 | 0.003 |
[11/28 11:32:17] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.003 | shadow     | 0.001 |
[11/28 11:32:17] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:32:17] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:32:17] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:32:17] d2.evaluation.testing INFO: copypaste: 0.0014,0.0081,0.0000,0.0000,0.0003,0.0029
[11/28 11:32:25] d2.utils.events INFO:  eta: 0:03:27  iter: 79  total_loss: 1.606  loss_cls: 0.1207  loss_box_reg: 0.0187  loss_rpn_cls: 0.6477  loss_rpn_loc: 0.825  validation_loss: 1.897  time: 0.8607  data_time: 0.0536  lr: 2.4763e-05  max_mem: 9915M
[11/28 11:32:40] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:32:40] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:32:40] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:32:40] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:32:40] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:32:40] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:32:40] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:32:40] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:32:40] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0317 s/iter. Eval: 0.0005 s/iter. Total: 0.0328 s/iter. ETA=0:00:05
[11/28 11:32:45] d2.evaluation.evaluator INFO: Inference done 162/184. Dataloading: 0.0009 s/iter. Inference: 0.0318 s/iter. Eval: 0.0005 s/iter. Total: 0.0332 s/iter. ETA=0:00:00
[11/28 11:32:46] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.957646 (0.033283 s / iter per device, on 1 devices)
[11/28 11:32:46] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031728 s / iter per device, on 1 devices)
[11/28 11:32:46] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:32:46] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:32:46] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:32:46] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:32:46] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.06 seconds.
[11/28 11:32:46] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:32:46] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 11:32:46] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.001 | 0.004  | 0.000  | 0.000 | 0.001 | 0.002 |
[11/28 11:32:46] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.001 | shadow     | 0.002 |
[11/28 11:32:46] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:32:46] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:32:46] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:32:46] d2.evaluation.testing INFO: copypaste: 0.0008,0.0039,0.0000,0.0000,0.0006,0.0017
[11/28 11:32:59] d2.utils.events INFO:  eta: 0:03:12  iter: 99  total_loss: 1.495  loss_cls: 0.09931  loss_box_reg: 0.01772  loss_rpn_cls: 0.6188  loss_rpn_loc: 0.7944  validation_loss: 1.85  time: 0.8715  data_time: 0.0560  lr: 3.1007e-05  max_mem: 9915M
[11/28 11:33:10] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:33:10] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:33:10] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:33:10] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:33:10] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:33:10] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:33:10] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:33:10] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:33:10] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0323 s/iter. Eval: 0.0004 s/iter. Total: 0.0333 s/iter. ETA=0:00:05
[11/28 11:33:15] d2.evaluation.evaluator INFO: Inference done 160/184. Dataloading: 0.0009 s/iter. Inference: 0.0322 s/iter. Eval: 0.0004 s/iter. Total: 0.0336 s/iter. ETA=0:00:00
[11/28 11:33:16] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.011236 (0.033582 s / iter per device, on 1 devices)
[11/28 11:33:16] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032124 s / iter per device, on 1 devices)
[11/28 11:33:16] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:33:16] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:33:16] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:33:16] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:33:16] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.03 seconds.
[11/28 11:33:16] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:33:16] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:33:16] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.001  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:33:16] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:33:16] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:33:16] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:33:16] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:33:16] d2.evaluation.testing INFO: copypaste: 0.0001,0.0006,0.0000,0.0000,0.0000,0.0001
[11/28 11:33:33] d2.utils.events INFO:  eta: 0:02:55  iter: 119  total_loss: 1.273  loss_cls: 0.07254  loss_box_reg: 0.01444  loss_rpn_cls: 0.5697  loss_rpn_loc: 0.6238  validation_loss: 1.803  time: 0.8856  data_time: 0.0520  lr: 3.725e-05  max_mem: 10277M
[11/28 11:33:41] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:33:41] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:33:41] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:33:41] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:33:41] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:33:41] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:33:41] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:33:41] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:33:42] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0331 s/iter. ETA=0:00:05
[11/28 11:33:47] d2.evaluation.evaluator INFO: Inference done 164/184. Dataloading: 0.0009 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0328 s/iter. ETA=0:00:00
[11/28 11:33:47] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.865357 (0.032767 s / iter per device, on 1 devices)
[11/28 11:33:47] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031506 s / iter per device, on 1 devices)
[11/28 11:33:47] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:33:47] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:33:47] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:33:47] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:33:47] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.01 seconds.
[11/28 11:33:47] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:33:47] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:33:47] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:33:47] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:33:47] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:33:47] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:33:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:33:47] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 11:34:10] d2.utils.events INFO:  eta: 0:02:40  iter: 139  total_loss: 1.055  loss_cls: 0.1152  loss_box_reg: 0.03478  loss_rpn_cls: 0.4831  loss_rpn_loc: 0.4104  validation_loss: 1.782  time: 0.9211  data_time: 0.0554  lr: 4.3494e-05  max_mem: 10277M
[11/28 11:34:15] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:34:15] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:34:15] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:34:15] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:34:15] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:34:15] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:34:15] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:34:15] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:34:15] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0308 s/iter. Eval: 0.0001 s/iter. Total: 0.0315 s/iter. ETA=0:00:05
[11/28 11:34:20] d2.evaluation.evaluator INFO: Inference done 168/184. Dataloading: 0.0010 s/iter. Inference: 0.0308 s/iter. Eval: 0.0001 s/iter. Total: 0.0319 s/iter. ETA=0:00:00
[11/28 11:34:21] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.730153 (0.032012 s / iter per device, on 1 devices)
[11/28 11:34:21] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.030725 s / iter per device, on 1 devices)
[11/28 11:34:21] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:34:21] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:34:21] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:34:21] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:34:21] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.10 seconds.
[11/28 11:34:21] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:34:21] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:34:21] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:34:21] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:34:21] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:34:21] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:34:21] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:34:21] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 11:34:48] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:34:48] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:34:48] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:34:48] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:34:48] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:34:48] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:34:48] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:34:48] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:34:49] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:00:05
[11/28 11:34:54] d2.evaluation.evaluator INFO: Inference done 158/184. Dataloading: 0.0009 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:00:00
[11/28 11:34:55] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.080321 (0.033968 s / iter per device, on 1 devices)
[11/28 11:34:55] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032807 s / iter per device, on 1 devices)
[11/28 11:34:55] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:34:55] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:34:55] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:34:55] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:34:55] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.01 seconds.
[11/28 11:34:55] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:34:55] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:34:55] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:34:55] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:34:55] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:34:55] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:34:55] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:34:55] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 11:35:04] d2.utils.events INFO:  eta: 0:02:23  iter: 159  total_loss: 1.036  loss_cls: 0.1146  loss_box_reg: 0.04192  loss_rpn_cls: 0.3992  loss_rpn_loc: 0.4736  validation_loss: 1.683  time: 0.9518  data_time: 0.0535  lr: 4.9738e-05  max_mem: 10277M
[11/28 11:35:22] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:35:22] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:35:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:35:22] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:35:22] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:35:22] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:35:22] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:35:22] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:35:23] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0325 s/iter. Eval: 0.0002 s/iter. Total: 0.0335 s/iter. ETA=0:00:05
[11/28 11:35:28] d2.evaluation.evaluator INFO: Inference done 163/184. Dataloading: 0.0009 s/iter. Inference: 0.0320 s/iter. Eval: 0.0002 s/iter. Total: 0.0331 s/iter. ETA=0:00:00
[11/28 11:35:28] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.924365 (0.033097 s / iter per device, on 1 devices)
[11/28 11:35:28] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031858 s / iter per device, on 1 devices)
[11/28 11:35:28] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:35:28] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:35:28] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:35:28] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:35:28] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.01 seconds.
[11/28 11:35:28] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:35:28] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:35:28] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:35:28] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:35:28] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:35:28] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:35:28] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:35:28] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 11:35:42] d2.utils.events INFO:  eta: 0:02:08  iter: 179  total_loss: 0.7958  loss_cls: 0.1033  loss_box_reg: 0.03833  loss_rpn_cls: 0.3106  loss_rpn_loc: 0.3568  validation_loss: 1.607  time: 0.9758  data_time: 0.0565  lr: 5.5982e-05  max_mem: 10277M
[11/28 11:35:56] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:35:56] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:35:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:35:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:35:56] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:35:56] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:35:56] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:35:56] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:35:56] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0332 s/iter. Eval: 0.0004 s/iter. Total: 0.0343 s/iter. ETA=0:00:05
[11/28 11:36:01] d2.evaluation.evaluator INFO: Inference done 159/184. Dataloading: 0.0009 s/iter. Inference: 0.0326 s/iter. Eval: 0.0003 s/iter. Total: 0.0339 s/iter. ETA=0:00:00
[11/28 11:36:02] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.077214 (0.033951 s / iter per device, on 1 devices)
[11/28 11:36:02] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032516 s / iter per device, on 1 devices)
[11/28 11:36:02] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:36:02] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:36:02] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:36:02] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:36:02] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:36:02] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:36:02] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:36:02] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:36:02] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:36:02] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:36:02] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:36:02] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:36:02] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 11:36:20] d2.utils.events INFO:  eta: 0:01:52  iter: 199  total_loss: 0.7031  loss_cls: 0.0944  loss_box_reg: 0.03738  loss_rpn_cls: 0.2431  loss_rpn_loc: 0.3344  validation_loss: 1.513  time: 0.9945  data_time: 0.0527  lr: 6.2225e-05  max_mem: 10277M
[11/28 11:36:29] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:36:29] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:36:29] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:36:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:36:29] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:36:29] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:36:29] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:36:29] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:36:30] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0335 s/iter. Eval: 0.0004 s/iter. Total: 0.0346 s/iter. ETA=0:00:05
[11/28 11:36:35] d2.evaluation.evaluator INFO: Inference done 156/184. Dataloading: 0.0009 s/iter. Inference: 0.0333 s/iter. Eval: 0.0004 s/iter. Total: 0.0346 s/iter. ETA=0:00:00
[11/28 11:36:36] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.187509 (0.034567 s / iter per device, on 1 devices)
[11/28 11:36:36] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.033103 s / iter per device, on 1 devices)
[11/28 11:36:36] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:36:36] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:36:36] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:36:36] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:36:36] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:36:36] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:36:36] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:36:36] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.001  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:36:36] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:36:36] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:36:36] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:36:36] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:36:36] d2.evaluation.testing INFO: copypaste: 0.0001,0.0007,0.0000,0.0000,0.0004,0.0000
[11/28 11:36:58] d2.utils.events INFO:  eta: 0:01:35  iter: 219  total_loss: 0.6996  loss_cls: 0.09487  loss_box_reg: 0.0424  loss_rpn_cls: 0.2168  loss_rpn_loc: 0.3431  validation_loss: 1.42  time: 1.0081  data_time: 0.0546  lr: 6.8469e-05  max_mem: 10277M
[11/28 11:37:03] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:37:03] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:37:03] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:37:03] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:37:03] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:37:03] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:37:03] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:37:03] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:37:03] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0004 s/iter. Total: 0.0339 s/iter. ETA=0:00:05
[11/28 11:37:08] d2.evaluation.evaluator INFO: Inference done 159/184. Dataloading: 0.0009 s/iter. Inference: 0.0325 s/iter. Eval: 0.0003 s/iter. Total: 0.0339 s/iter. ETA=0:00:00
[11/28 11:37:09] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.073292 (0.033929 s / iter per device, on 1 devices)
[11/28 11:37:09] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032482 s / iter per device, on 1 devices)
[11/28 11:37:09] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:37:09] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:37:09] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:37:09] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:37:09] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:37:09] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:37:09] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:37:09] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 11:37:09] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:37:09] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:37:09] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:37:09] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:37:09] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 11:37:36] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:37:36] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:37:36] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:37:36] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:37:36] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:37:36] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:37:36] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:37:36] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:37:37] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0325 s/iter. Eval: 0.0004 s/iter. Total: 0.0335 s/iter. ETA=0:00:05
[11/28 11:37:42] d2.evaluation.evaluator INFO: Inference done 160/184. Dataloading: 0.0009 s/iter. Inference: 0.0324 s/iter. Eval: 0.0004 s/iter. Total: 0.0337 s/iter. ETA=0:00:00
[11/28 11:37:42] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.049524 (0.033796 s / iter per device, on 1 devices)
[11/28 11:37:42] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032308 s / iter per device, on 1 devices)
[11/28 11:37:42] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:37:42] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:37:42] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:37:42] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:37:42] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:37:42] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:37:42] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:37:42] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.002 | 0.007  | 0.000  | 0.000 | 0.008 | 0.000 |
[11/28 11:37:42] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.007 | disc       | 0.000 | shadow     | 0.000 |
[11/28 11:37:42] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:37:42] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:37:42] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:37:42] d2.evaluation.testing INFO: copypaste: 0.0023,0.0070,0.0000,0.0000,0.0077,0.0000
[11/28 11:37:51] d2.utils.events INFO:  eta: 0:01:24  iter: 239  total_loss: 0.6731  loss_cls: 0.08787  loss_box_reg: 0.03987  loss_rpn_cls: 0.1973  loss_rpn_loc: 0.3481  validation_loss: 1.236  time: 1.0195  data_time: 0.0552  lr: 7.4713e-05  max_mem: 10277M
[11/28 11:38:09] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:38:09] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:38:09] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:38:09] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:38:09] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:38:10] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:38:10] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:38:10] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:38:10] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0330 s/iter. Eval: 0.0004 s/iter. Total: 0.0340 s/iter. ETA=0:00:05
[11/28 11:38:15] d2.evaluation.evaluator INFO: Inference done 155/184. Dataloading: 0.0009 s/iter. Inference: 0.0334 s/iter. Eval: 0.0004 s/iter. Total: 0.0348 s/iter. ETA=0:00:01
[11/28 11:38:16] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.236794 (0.034842 s / iter per device, on 1 devices)
[11/28 11:38:16] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.033335 s / iter per device, on 1 devices)
[11/28 11:38:16] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:38:16] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:38:16] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:38:16] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:38:16] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:38:16] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:38:16] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:38:16] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.012 | 0.043  | 0.000  | 0.000 | 0.006 | 0.020 |
[11/28 11:38:16] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.009 | disc       | 0.027 | shadow     | 0.000 |
[11/28 11:38:16] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:38:16] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:38:16] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:38:16] d2.evaluation.testing INFO: copypaste: 0.0121,0.0427,0.0000,0.0000,0.0058,0.0195
[11/28 11:38:29] d2.utils.events INFO:  eta: 0:01:04  iter: 259  total_loss: 0.6669  loss_cls: 0.09095  loss_box_reg: 0.04516  loss_rpn_cls: 0.1842  loss_rpn_loc: 0.334  validation_loss: 1.175  time: 1.0287  data_time: 0.0554  lr: 8.0957e-05  max_mem: 10277M
[11/28 11:38:43] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:38:43] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:38:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:38:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:38:43] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:38:43] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:38:43] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:38:43] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:38:44] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0340 s/iter. Eval: 0.0004 s/iter. Total: 0.0351 s/iter. ETA=0:00:06
[11/28 11:38:49] d2.evaluation.evaluator INFO: Inference done 158/184. Dataloading: 0.0009 s/iter. Inference: 0.0329 s/iter. Eval: 0.0003 s/iter. Total: 0.0342 s/iter. ETA=0:00:00
[11/28 11:38:50] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.117607 (0.034177 s / iter per device, on 1 devices)
[11/28 11:38:50] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032757 s / iter per device, on 1 devices)
[11/28 11:38:50] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:38:50] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:38:50] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:38:50] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:38:50] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:38:50] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:38:50] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:38:50] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.279 | 0.850  | 0.330  | 0.000 | 0.001 | 0.291 |
[11/28 11:38:50] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.001 | disc       | 0.836 | shadow     | 0.000 |
[11/28 11:38:50] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:38:50] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:38:50] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:38:50] d2.evaluation.testing INFO: copypaste: 0.2791,0.8504,0.3300,0.0000,0.0014,0.2915
[11/28 11:39:07] d2.utils.events INFO:  eta: 0:00:43  iter: 279  total_loss: 0.5949  loss_cls: 0.07647  loss_box_reg: 0.03503  loss_rpn_cls: 0.1893  loss_rpn_loc: 0.2799  validation_loss: 1.115  time: 1.0384  data_time: 0.0540  lr: 8.72e-05  max_mem: 10277M
[11/28 11:39:17] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:39:17] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:39:17] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:39:17] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:39:17] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:39:17] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:39:17] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:39:17] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:39:17] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0318 s/iter. Eval: 0.0003 s/iter. Total: 0.0328 s/iter. ETA=0:00:05
[11/28 11:39:22] d2.evaluation.evaluator INFO: Inference done 162/184. Dataloading: 0.0010 s/iter. Inference: 0.0318 s/iter. Eval: 0.0003 s/iter. Total: 0.0332 s/iter. ETA=0:00:00
[11/28 11:39:23] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.953855 (0.033262 s / iter per device, on 1 devices)
[11/28 11:39:23] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031740 s / iter per device, on 1 devices)
[11/28 11:39:23] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:39:23] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:39:23] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:39:23] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:39:23] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:39:23] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:39:23] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:39:23] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.588 | 2.278  | 0.456  | 0.000 | 0.052 | 0.650 |
[11/28 11:39:23] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.011 | disc       | 1.754 | shadow     | 0.000 |
[11/28 11:39:23] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:39:23] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:39:23] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:39:23] d2.evaluation.testing INFO: copypaste: 0.5883,2.2784,0.4563,0.0000,0.0516,0.6496
[11/28 11:39:45] d2.utils.events INFO:  eta: 0:00:22  iter: 299  total_loss: 0.6166  loss_cls: 0.08598  loss_box_reg: 0.04362  loss_rpn_cls: 0.1811  loss_rpn_loc: 0.3013  validation_loss: 1.064  time: 1.0457  data_time: 0.0538  lr: 9.3444e-05  max_mem: 10279M
[11/28 11:39:50] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:39:50] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:39:50] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:39:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:39:50] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:39:50] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:39:50] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:39:50] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:39:51] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0336 s/iter. Eval: 0.0004 s/iter. Total: 0.0347 s/iter. ETA=0:00:06
[11/28 11:39:56] d2.evaluation.evaluator INFO: Inference done 159/184. Dataloading: 0.0009 s/iter. Inference: 0.0326 s/iter. Eval: 0.0004 s/iter. Total: 0.0340 s/iter. ETA=0:00:00
[11/28 11:39:56] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.085507 (0.033997 s / iter per device, on 1 devices)
[11/28 11:39:56] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032530 s / iter per device, on 1 devices)
[11/28 11:39:56] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:39:56] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:39:56] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:39:57] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:39:57] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 11:39:57] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:39:57] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:39:57] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 1.170 | 5.383  | 0.441  | 0.000 | 0.982 | 1.202 |
[11/28 11:39:57] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 3.509 | shadow     | 0.000 |
[11/28 11:39:57] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:39:57] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:39:57] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:39:57] d2.evaluation.testing INFO: copypaste: 1.1697,5.3828,0.4408,0.0000,0.9822,1.2018
[11/28 11:40:24] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/trial-2022-11-28-11-29-55/model_final.pth
[11/28 11:40:33] d2.utils.events INFO:  eta: 0:00:00  iter: 319  total_loss: 0.6403  loss_cls: 0.08833  loss_box_reg: 0.04615  loss_rpn_cls: 0.1837  loss_rpn_loc: 0.2926  validation_loss: 0.9579  time: 1.0520  data_time: 0.0539  lr: 9.9688e-05  max_mem: 10279M
[11/28 11:40:33] d2.engine.hooks INFO: Overall training speed: 318 iterations in 0:05:34 (1.0520 s / it)
[11/28 11:40:33] d2.engine.hooks INFO: Total training time: 0:10:31 (0:04:56 on hooks)
[11/28 11:40:33] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 11:40:33] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 11:40:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 11:40:33] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 11:40:33] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 11:40:33] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 11:40:33] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 11:40:33] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 11:40:34] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0008 s/iter. Inference: 0.0341 s/iter. Eval: 0.0004 s/iter. Total: 0.0353 s/iter. ETA=0:00:06
[11/28 11:40:39] d2.evaluation.evaluator INFO: Inference done 154/184. Dataloading: 0.0009 s/iter. Inference: 0.0337 s/iter. Eval: 0.0004 s/iter. Total: 0.0350 s/iter. ETA=0:00:01
[11/28 11:40:40] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.257642 (0.034959 s / iter per device, on 1 devices)
[11/28 11:40:40] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.033482 s / iter per device, on 1 devices)
[11/28 11:40:40] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 11:40:40] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/inference/coco_instances_results.json
[11/28 11:40:40] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 11:40:40] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 11:40:40] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.03 seconds.
[11/28 11:40:40] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 11:40:40] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 11:40:40] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 1.254 | 5.710  | 0.354  | 0.000 | 0.380 | 1.512 |
[11/28 11:40:40] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.001 | disc       | 3.721 | shadow     | 0.040 |
[11/28 11:40:40] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 11:40:40] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 11:40:40] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 11:40:40] d2.evaluation.testing INFO: copypaste: 1.2538,5.7104,0.3545,0.0000,0.3798,1.5122
[11/28 11:40:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/trial-2022-11-28-11-11-44/model_final.pth ...
[11/28 11:41:21] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/trial-2022-11-28-11-29-55/model_final.pth ...
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.anchor_deltas.bias in checkpoint is torch.Size([12]), while shape of proposal_generator.rpn_head.anchor_deltas.bias in model is torch.Size([60]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.anchor_deltas.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.anchor_deltas.weight in checkpoint is torch.Size([12, 256, 1, 1]), while shape of proposal_generator.rpn_head.anchor_deltas.weight in model is torch.Size([60, 1024, 1, 1]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.anchor_deltas.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.conv.bias in checkpoint is torch.Size([256]), while shape of proposal_generator.rpn_head.conv.bias in model is torch.Size([1024]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.conv.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.conv.weight in checkpoint is torch.Size([256, 256, 3, 3]), while shape of proposal_generator.rpn_head.conv.weight in model is torch.Size([1024, 1024, 3, 3]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.conv.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.objectness_logits.bias in checkpoint is torch.Size([3]), while shape of proposal_generator.rpn_head.objectness_logits.bias in model is torch.Size([15]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.objectness_logits.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.objectness_logits.weight in checkpoint is torch.Size([3, 256, 1, 1]), while shape of proposal_generator.rpn_head.objectness_logits.weight in model is torch.Size([15, 1024, 1, 1]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.objectness_logits.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.bbox_pred.bias in checkpoint is torch.Size([12]), while shape of roi_heads.box_predictor.bbox_pred.bias in model is torch.Size([320]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.bbox_pred.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.bbox_pred.weight in checkpoint is torch.Size([12, 1024]), while shape of roi_heads.box_predictor.bbox_pred.weight in model is torch.Size([320, 2048]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.bbox_pred.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.cls_score.bias in checkpoint is torch.Size([4]), while shape of roi_heads.box_predictor.cls_score.bias in model is torch.Size([81]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.cls_score.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.cls_score.weight in checkpoint is torch.Size([4, 1024]), while shape of roi_heads.box_predictor.cls_score.weight in model is torch.Size([81, 2048]).
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.cls_score.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:41:21] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.conv.weight' to the model due to incompatible shapes: (256, 256, 3, 3) in the checkpoint but (1024, 1024, 3, 3) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.conv.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.weight' to the model due to incompatible shapes: (3, 256, 1, 1) in the checkpoint but (15, 1024, 1, 1) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.bias' to the model due to incompatible shapes: (3,) in the checkpoint but (15,) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (60, 1024, 1, 1) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (60,) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (4, 1024) in the checkpoint but (81, 2048) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (4,) in the checkpoint but (81,) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (12, 1024) in the checkpoint but (320, 2048) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (320,) in the model! You might want to double check if this is expected.
[11/28 11:41:21] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
backbone.res2.0.conv1.norm.{bias, weight}
backbone.res2.0.conv1.weight
backbone.res2.0.conv2.norm.{bias, weight}
backbone.res2.0.conv2.weight
backbone.res2.0.conv3.norm.{bias, weight}
backbone.res2.0.conv3.weight
backbone.res2.0.shortcut.norm.{bias, weight}
backbone.res2.0.shortcut.weight
backbone.res2.1.conv1.norm.{bias, weight}
backbone.res2.1.conv1.weight
backbone.res2.1.conv2.norm.{bias, weight}
backbone.res2.1.conv2.weight
backbone.res2.1.conv3.norm.{bias, weight}
backbone.res2.1.conv3.weight
backbone.res2.2.conv1.norm.{bias, weight}
backbone.res2.2.conv1.weight
backbone.res2.2.conv2.norm.{bias, weight}
backbone.res2.2.conv2.weight
backbone.res2.2.conv3.norm.{bias, weight}
backbone.res2.2.conv3.weight
backbone.res3.0.conv1.norm.{bias, weight}
backbone.res3.0.conv1.weight
backbone.res3.0.conv2.norm.{bias, weight}
backbone.res3.0.conv2.weight
backbone.res3.0.conv3.norm.{bias, weight}
backbone.res3.0.conv3.weight
backbone.res3.0.shortcut.norm.{bias, weight}
backbone.res3.0.shortcut.weight
backbone.res3.1.conv1.norm.{bias, weight}
backbone.res3.1.conv1.weight
backbone.res3.1.conv2.norm.{bias, weight}
backbone.res3.1.conv2.weight
backbone.res3.1.conv3.norm.{bias, weight}
backbone.res3.1.conv3.weight
backbone.res3.2.conv1.norm.{bias, weight}
backbone.res3.2.conv1.weight
backbone.res3.2.conv2.norm.{bias, weight}
backbone.res3.2.conv2.weight
backbone.res3.2.conv3.norm.{bias, weight}
backbone.res3.2.conv3.weight
backbone.res3.3.conv1.norm.{bias, weight}
backbone.res3.3.conv1.weight
backbone.res3.3.conv2.norm.{bias, weight}
backbone.res3.3.conv2.weight
backbone.res3.3.conv3.norm.{bias, weight}
backbone.res3.3.conv3.weight
backbone.res4.0.conv1.norm.{bias, weight}
backbone.res4.0.conv1.weight
backbone.res4.0.conv2.norm.{bias, weight}
backbone.res4.0.conv2.weight
backbone.res4.0.conv3.norm.{bias, weight}
backbone.res4.0.conv3.weight
backbone.res4.0.shortcut.norm.{bias, weight}
backbone.res4.0.shortcut.weight
backbone.res4.1.conv1.norm.{bias, weight}
backbone.res4.1.conv1.weight
backbone.res4.1.conv2.norm.{bias, weight}
backbone.res4.1.conv2.weight
backbone.res4.1.conv3.norm.{bias, weight}
backbone.res4.1.conv3.weight
backbone.res4.2.conv1.norm.{bias, weight}
backbone.res4.2.conv1.weight
backbone.res4.2.conv2.norm.{bias, weight}
backbone.res4.2.conv2.weight
backbone.res4.2.conv3.norm.{bias, weight}
backbone.res4.2.conv3.weight
backbone.res4.3.conv1.norm.{bias, weight}
backbone.res4.3.conv1.weight
backbone.res4.3.conv2.norm.{bias, weight}
backbone.res4.3.conv2.weight
backbone.res4.3.conv3.norm.{bias, weight}
backbone.res4.3.conv3.weight
backbone.res4.4.conv1.norm.{bias, weight}
backbone.res4.4.conv1.weight
backbone.res4.4.conv2.norm.{bias, weight}
backbone.res4.4.conv2.weight
backbone.res4.4.conv3.norm.{bias, weight}
backbone.res4.4.conv3.weight
backbone.res4.5.conv1.norm.{bias, weight}
backbone.res4.5.conv1.weight
backbone.res4.5.conv2.norm.{bias, weight}
backbone.res4.5.conv2.weight
backbone.res4.5.conv3.norm.{bias, weight}
backbone.res4.5.conv3.weight
backbone.stem.conv1.norm.{bias, weight}
backbone.stem.conv1.weight
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_predictor.bbox_pred.{bias, weight}
roi_heads.box_predictor.cls_score.{bias, weight}
roi_heads.res5.0.conv1.norm.{bias, weight}
roi_heads.res5.0.conv1.weight
roi_heads.res5.0.conv2.norm.{bias, weight}
roi_heads.res5.0.conv2.weight
roi_heads.res5.0.conv3.norm.{bias, weight}
roi_heads.res5.0.conv3.weight
roi_heads.res5.0.shortcut.norm.{bias, weight}
roi_heads.res5.0.shortcut.weight
roi_heads.res5.1.conv1.norm.{bias, weight}
roi_heads.res5.1.conv1.weight
roi_heads.res5.1.conv2.norm.{bias, weight}
roi_heads.res5.1.conv2.weight
roi_heads.res5.1.conv3.norm.{bias, weight}
roi_heads.res5.1.conv3.weight
roi_heads.res5.2.conv1.norm.{bias, weight}
roi_heads.res5.2.conv1.weight
roi_heads.res5.2.conv2.norm.{bias, weight}
roi_heads.res5.2.conv2.weight
roi_heads.res5.2.conv3.norm.{bias, weight}
roi_heads.res5.2.conv3.weight
[11/28 11:41:21] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  backbone.fpn_lateral2.{bias, weight}
  backbone.fpn_output2.{bias, weight}
  backbone.fpn_lateral3.{bias, weight}
  backbone.fpn_output3.{bias, weight}
  backbone.fpn_lateral4.{bias, weight}
  backbone.fpn_output4.{bias, weight}
  backbone.fpn_lateral5.{bias, weight}
  backbone.fpn_output5.{bias, weight}
  backbone.bottom_up.stem.conv1.weight
  backbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.shortcut.weight
  backbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv1.weight
  backbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv2.weight
  backbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv3.weight
  backbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv1.weight
  backbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv2.weight
  backbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv3.weight
  backbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv1.weight
  backbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv2.weight
  backbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv3.weight
  backbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.shortcut.weight
  backbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv1.weight
  backbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv2.weight
  backbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv3.weight
  backbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv1.weight
  backbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv2.weight
  backbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv3.weight
  backbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv1.weight
  backbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv2.weight
  backbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv3.weight
  backbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv1.weight
  backbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv2.weight
  backbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv3.weight
  backbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.shortcut.weight
  backbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv1.weight
  backbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv2.weight
  backbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv3.weight
  backbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv1.weight
  backbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv2.weight
  backbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv3.weight
  backbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv1.weight
  backbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv2.weight
  backbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv3.weight
  backbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv1.weight
  backbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv2.weight
  backbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv3.weight
  backbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv1.weight
  backbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv2.weight
  backbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv3.weight
  backbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv1.weight
  backbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv2.weight
  backbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv3.weight
  backbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.shortcut.weight
  backbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv1.weight
  backbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv2.weight
  backbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv3.weight
  backbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv1.weight
  backbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv2.weight
  backbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv3.weight
  backbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv1.weight
  backbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv2.weight
  backbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv3.weight
  backbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}
  roi_heads.box_head.fc1.{bias, weight}
  roi_heads.box_head.fc2.{bias, weight}
[11/28 11:55:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/trial-2022-11-28-11-29-55/model_final.pth ...
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.anchor_deltas.bias in checkpoint is torch.Size([12]), while shape of proposal_generator.rpn_head.anchor_deltas.bias in model is torch.Size([60]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.anchor_deltas.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.anchor_deltas.weight in checkpoint is torch.Size([12, 256, 1, 1]), while shape of proposal_generator.rpn_head.anchor_deltas.weight in model is torch.Size([60, 1024, 1, 1]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.anchor_deltas.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.conv.bias in checkpoint is torch.Size([256]), while shape of proposal_generator.rpn_head.conv.bias in model is torch.Size([1024]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.conv.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.conv.weight in checkpoint is torch.Size([256, 256, 3, 3]), while shape of proposal_generator.rpn_head.conv.weight in model is torch.Size([1024, 1024, 3, 3]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.conv.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.objectness_logits.bias in checkpoint is torch.Size([3]), while shape of proposal_generator.rpn_head.objectness_logits.bias in model is torch.Size([15]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.objectness_logits.bias will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.objectness_logits.weight in checkpoint is torch.Size([3, 256, 1, 1]), while shape of proposal_generator.rpn_head.objectness_logits.weight in model is torch.Size([15, 1024, 1, 1]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.objectness_logits.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.bbox_pred.weight in checkpoint is torch.Size([12, 1024]), while shape of roi_heads.box_predictor.bbox_pred.weight in model is torch.Size([12, 2048]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.bbox_pred.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.cls_score.weight in checkpoint is torch.Size([4, 1024]), while shape of roi_heads.box_predictor.cls_score.weight in model is torch.Size([4, 2048]).
[11/28 11:55:16] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.cls_score.weight will not be loaded. Please double check and see if this is desired.
[11/28 11:55:16] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule roi_heads.box_predictor.bias:
| Names in Model   | Names in Checkpoint                    | Shapes   |
|:-----------------|:---------------------------------------|:---------|
| pred.bias        | roi_heads.box_predictor.bbox_pred.bias | (12,)    |
| core.bias        | roi_heads.box_predictor.cls_score.bias | (4,)     |
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (60,) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (60, 1024, 1, 1) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.conv.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.conv.weight' to the model due to incompatible shapes: (256, 256, 3, 3) in the checkpoint but (1024, 1024, 3, 3) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.bias' to the model due to incompatible shapes: (3,) in the checkpoint but (15,) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.weight' to the model due to incompatible shapes: (3, 256, 1, 1) in the checkpoint but (15, 1024, 1, 1) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (12, 1024) in the checkpoint but (12, 2048) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (4, 1024) in the checkpoint but (4, 2048) in the model! You might want to double check if this is expected.
[11/28 11:55:16] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
backbone.res2.0.conv1.norm.{bias, weight}
backbone.res2.0.conv1.weight
backbone.res2.0.conv2.norm.{bias, weight}
backbone.res2.0.conv2.weight
backbone.res2.0.conv3.norm.{bias, weight}
backbone.res2.0.conv3.weight
backbone.res2.0.shortcut.norm.{bias, weight}
backbone.res2.0.shortcut.weight
backbone.res2.1.conv1.norm.{bias, weight}
backbone.res2.1.conv1.weight
backbone.res2.1.conv2.norm.{bias, weight}
backbone.res2.1.conv2.weight
backbone.res2.1.conv3.norm.{bias, weight}
backbone.res2.1.conv3.weight
backbone.res2.2.conv1.norm.{bias, weight}
backbone.res2.2.conv1.weight
backbone.res2.2.conv2.norm.{bias, weight}
backbone.res2.2.conv2.weight
backbone.res2.2.conv3.norm.{bias, weight}
backbone.res2.2.conv3.weight
backbone.res3.0.conv1.norm.{bias, weight}
backbone.res3.0.conv1.weight
backbone.res3.0.conv2.norm.{bias, weight}
backbone.res3.0.conv2.weight
backbone.res3.0.conv3.norm.{bias, weight}
backbone.res3.0.conv3.weight
backbone.res3.0.shortcut.norm.{bias, weight}
backbone.res3.0.shortcut.weight
backbone.res3.1.conv1.norm.{bias, weight}
backbone.res3.1.conv1.weight
backbone.res3.1.conv2.norm.{bias, weight}
backbone.res3.1.conv2.weight
backbone.res3.1.conv3.norm.{bias, weight}
backbone.res3.1.conv3.weight
backbone.res3.2.conv1.norm.{bias, weight}
backbone.res3.2.conv1.weight
backbone.res3.2.conv2.norm.{bias, weight}
backbone.res3.2.conv2.weight
backbone.res3.2.conv3.norm.{bias, weight}
backbone.res3.2.conv3.weight
backbone.res3.3.conv1.norm.{bias, weight}
backbone.res3.3.conv1.weight
backbone.res3.3.conv2.norm.{bias, weight}
backbone.res3.3.conv2.weight
backbone.res3.3.conv3.norm.{bias, weight}
backbone.res3.3.conv3.weight
backbone.res4.0.conv1.norm.{bias, weight}
backbone.res4.0.conv1.weight
backbone.res4.0.conv2.norm.{bias, weight}
backbone.res4.0.conv2.weight
backbone.res4.0.conv3.norm.{bias, weight}
backbone.res4.0.conv3.weight
backbone.res4.0.shortcut.norm.{bias, weight}
backbone.res4.0.shortcut.weight
backbone.res4.1.conv1.norm.{bias, weight}
backbone.res4.1.conv1.weight
backbone.res4.1.conv2.norm.{bias, weight}
backbone.res4.1.conv2.weight
backbone.res4.1.conv3.norm.{bias, weight}
backbone.res4.1.conv3.weight
backbone.res4.2.conv1.norm.{bias, weight}
backbone.res4.2.conv1.weight
backbone.res4.2.conv2.norm.{bias, weight}
backbone.res4.2.conv2.weight
backbone.res4.2.conv3.norm.{bias, weight}
backbone.res4.2.conv3.weight
backbone.res4.3.conv1.norm.{bias, weight}
backbone.res4.3.conv1.weight
backbone.res4.3.conv2.norm.{bias, weight}
backbone.res4.3.conv2.weight
backbone.res4.3.conv3.norm.{bias, weight}
backbone.res4.3.conv3.weight
backbone.res4.4.conv1.norm.{bias, weight}
backbone.res4.4.conv1.weight
backbone.res4.4.conv2.norm.{bias, weight}
backbone.res4.4.conv2.weight
backbone.res4.4.conv3.norm.{bias, weight}
backbone.res4.4.conv3.weight
backbone.res4.5.conv1.norm.{bias, weight}
backbone.res4.5.conv1.weight
backbone.res4.5.conv2.norm.{bias, weight}
backbone.res4.5.conv2.weight
backbone.res4.5.conv3.norm.{bias, weight}
backbone.res4.5.conv3.weight
backbone.stem.conv1.norm.{bias, weight}
backbone.stem.conv1.weight
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_predictor.bbox_pred.weight
roi_heads.box_predictor.cls_score.weight
roi_heads.res5.0.conv1.norm.{bias, weight}
roi_heads.res5.0.conv1.weight
roi_heads.res5.0.conv2.norm.{bias, weight}
roi_heads.res5.0.conv2.weight
roi_heads.res5.0.conv3.norm.{bias, weight}
roi_heads.res5.0.conv3.weight
roi_heads.res5.0.shortcut.norm.{bias, weight}
roi_heads.res5.0.shortcut.weight
roi_heads.res5.1.conv1.norm.{bias, weight}
roi_heads.res5.1.conv1.weight
roi_heads.res5.1.conv2.norm.{bias, weight}
roi_heads.res5.1.conv2.weight
roi_heads.res5.1.conv3.norm.{bias, weight}
roi_heads.res5.1.conv3.weight
roi_heads.res5.2.conv1.norm.{bias, weight}
roi_heads.res5.2.conv1.weight
roi_heads.res5.2.conv2.norm.{bias, weight}
roi_heads.res5.2.conv2.weight
roi_heads.res5.2.conv3.norm.{bias, weight}
roi_heads.res5.2.conv3.weight
[11/28 11:55:16] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  backbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv1.weight
  backbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv2.weight
  backbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv3.weight
  backbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.shortcut.weight
  backbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv1.weight
  backbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv2.weight
  backbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv3.weight
  backbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv1.weight
  backbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv2.weight
  backbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv3.weight
  backbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv1.weight
  backbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv2.weight
  backbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv3.weight
  backbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.shortcut.weight
  backbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv1.weight
  backbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv2.weight
  backbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv3.weight
  backbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv1.weight
  backbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv2.weight
  backbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv3.weight
  backbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv1.weight
  backbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv2.weight
  backbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv3.weight
  backbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv1.weight
  backbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv2.weight
  backbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv3.weight
  backbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.shortcut.weight
  backbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv1.weight
  backbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv2.weight
  backbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv3.weight
  backbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv1.weight
  backbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv2.weight
  backbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv3.weight
  backbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv1.weight
  backbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv2.weight
  backbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv3.weight
  backbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv1.weight
  backbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv2.weight
  backbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv3.weight
  backbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv1.weight
  backbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv2.weight
  backbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv3.weight
  backbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv1.weight
  backbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv2.weight
  backbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv3.weight
  backbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.shortcut.weight
  backbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv1.weight
  backbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv2.weight
  backbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv3.weight
  backbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv1.weight
  backbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv2.weight
  backbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv3.weight
  backbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.stem.conv1.weight
  backbone.fpn_lateral2.{bias, weight}
  backbone.fpn_lateral3.{bias, weight}
  backbone.fpn_lateral4.{bias, weight}
  backbone.fpn_lateral5.{bias, weight}
  backbone.fpn_output2.{bias, weight}
  backbone.fpn_output3.{bias, weight}
  backbone.fpn_output4.{bias, weight}
  backbone.fpn_output5.{bias, weight}
  roi_heads.box_head.fc1.{bias, weight}
  roi_heads.box_head.fc2.{bias, weight}
[11/28 12:01:12] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/trial-2022-11-28-11-29-55/model_final.pth ...
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.anchor_deltas.bias in checkpoint is torch.Size([12]), while shape of proposal_generator.rpn_head.anchor_deltas.bias in model is torch.Size([60]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.anchor_deltas.bias will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.anchor_deltas.weight in checkpoint is torch.Size([12, 256, 1, 1]), while shape of proposal_generator.rpn_head.anchor_deltas.weight in model is torch.Size([60, 1024, 1, 1]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.anchor_deltas.weight will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.conv.bias in checkpoint is torch.Size([256]), while shape of proposal_generator.rpn_head.conv.bias in model is torch.Size([1024]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.conv.bias will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.conv.weight in checkpoint is torch.Size([256, 256, 3, 3]), while shape of proposal_generator.rpn_head.conv.weight in model is torch.Size([1024, 1024, 3, 3]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.conv.weight will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.objectness_logits.bias in checkpoint is torch.Size([3]), while shape of proposal_generator.rpn_head.objectness_logits.bias in model is torch.Size([15]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.objectness_logits.bias will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of proposal_generator.rpn_head.objectness_logits.weight in checkpoint is torch.Size([3, 256, 1, 1]), while shape of proposal_generator.rpn_head.objectness_logits.weight in model is torch.Size([15, 1024, 1, 1]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: proposal_generator.rpn_head.objectness_logits.weight will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.bbox_pred.weight in checkpoint is torch.Size([12, 1024]), while shape of roi_heads.box_predictor.bbox_pred.weight in model is torch.Size([12, 2048]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.bbox_pred.weight will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: Shape of roi_heads.box_predictor.cls_score.weight in checkpoint is torch.Size([4, 1024]), while shape of roi_heads.box_predictor.cls_score.weight in model is torch.Size([4, 2048]).
[11/28 12:01:12] d2.checkpoint.c2_model_loading WARNING: roi_heads.box_predictor.cls_score.weight will not be loaded. Please double check and see if this is desired.
[11/28 12:01:12] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule roi_heads.box_predictor.bias:
| Names in Model   | Names in Checkpoint                    | Shapes   |
|:-----------------|:---------------------------------------|:---------|
| pred.bias        | roi_heads.box_predictor.bbox_pred.bias | (12,)    |
| core.bias        | roi_heads.box_predictor.cls_score.bias | (4,)     |
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (60,) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (60, 1024, 1, 1) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.conv.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.conv.weight' to the model due to incompatible shapes: (256, 256, 3, 3) in the checkpoint but (1024, 1024, 3, 3) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.bias' to the model due to incompatible shapes: (3,) in the checkpoint but (15,) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.weight' to the model due to incompatible shapes: (3, 256, 1, 1) in the checkpoint but (15, 1024, 1, 1) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (12, 1024) in the checkpoint but (12, 2048) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (4, 1024) in the checkpoint but (4, 2048) in the model! You might want to double check if this is expected.
[11/28 12:01:12] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
backbone.res2.0.conv1.norm.{bias, weight}
backbone.res2.0.conv1.weight
backbone.res2.0.conv2.norm.{bias, weight}
backbone.res2.0.conv2.weight
backbone.res2.0.conv3.norm.{bias, weight}
backbone.res2.0.conv3.weight
backbone.res2.0.shortcut.norm.{bias, weight}
backbone.res2.0.shortcut.weight
backbone.res2.1.conv1.norm.{bias, weight}
backbone.res2.1.conv1.weight
backbone.res2.1.conv2.norm.{bias, weight}
backbone.res2.1.conv2.weight
backbone.res2.1.conv3.norm.{bias, weight}
backbone.res2.1.conv3.weight
backbone.res2.2.conv1.norm.{bias, weight}
backbone.res2.2.conv1.weight
backbone.res2.2.conv2.norm.{bias, weight}
backbone.res2.2.conv2.weight
backbone.res2.2.conv3.norm.{bias, weight}
backbone.res2.2.conv3.weight
backbone.res3.0.conv1.norm.{bias, weight}
backbone.res3.0.conv1.weight
backbone.res3.0.conv2.norm.{bias, weight}
backbone.res3.0.conv2.weight
backbone.res3.0.conv3.norm.{bias, weight}
backbone.res3.0.conv3.weight
backbone.res3.0.shortcut.norm.{bias, weight}
backbone.res3.0.shortcut.weight
backbone.res3.1.conv1.norm.{bias, weight}
backbone.res3.1.conv1.weight
backbone.res3.1.conv2.norm.{bias, weight}
backbone.res3.1.conv2.weight
backbone.res3.1.conv3.norm.{bias, weight}
backbone.res3.1.conv3.weight
backbone.res3.2.conv1.norm.{bias, weight}
backbone.res3.2.conv1.weight
backbone.res3.2.conv2.norm.{bias, weight}
backbone.res3.2.conv2.weight
backbone.res3.2.conv3.norm.{bias, weight}
backbone.res3.2.conv3.weight
backbone.res3.3.conv1.norm.{bias, weight}
backbone.res3.3.conv1.weight
backbone.res3.3.conv2.norm.{bias, weight}
backbone.res3.3.conv2.weight
backbone.res3.3.conv3.norm.{bias, weight}
backbone.res3.3.conv3.weight
backbone.res4.0.conv1.norm.{bias, weight}
backbone.res4.0.conv1.weight
backbone.res4.0.conv2.norm.{bias, weight}
backbone.res4.0.conv2.weight
backbone.res4.0.conv3.norm.{bias, weight}
backbone.res4.0.conv3.weight
backbone.res4.0.shortcut.norm.{bias, weight}
backbone.res4.0.shortcut.weight
backbone.res4.1.conv1.norm.{bias, weight}
backbone.res4.1.conv1.weight
backbone.res4.1.conv2.norm.{bias, weight}
backbone.res4.1.conv2.weight
backbone.res4.1.conv3.norm.{bias, weight}
backbone.res4.1.conv3.weight
backbone.res4.2.conv1.norm.{bias, weight}
backbone.res4.2.conv1.weight
backbone.res4.2.conv2.norm.{bias, weight}
backbone.res4.2.conv2.weight
backbone.res4.2.conv3.norm.{bias, weight}
backbone.res4.2.conv3.weight
backbone.res4.3.conv1.norm.{bias, weight}
backbone.res4.3.conv1.weight
backbone.res4.3.conv2.norm.{bias, weight}
backbone.res4.3.conv2.weight
backbone.res4.3.conv3.norm.{bias, weight}
backbone.res4.3.conv3.weight
backbone.res4.4.conv1.norm.{bias, weight}
backbone.res4.4.conv1.weight
backbone.res4.4.conv2.norm.{bias, weight}
backbone.res4.4.conv2.weight
backbone.res4.4.conv3.norm.{bias, weight}
backbone.res4.4.conv3.weight
backbone.res4.5.conv1.norm.{bias, weight}
backbone.res4.5.conv1.weight
backbone.res4.5.conv2.norm.{bias, weight}
backbone.res4.5.conv2.weight
backbone.res4.5.conv3.norm.{bias, weight}
backbone.res4.5.conv3.weight
backbone.stem.conv1.norm.{bias, weight}
backbone.stem.conv1.weight
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_predictor.bbox_pred.weight
roi_heads.box_predictor.cls_score.weight
roi_heads.res5.0.conv1.norm.{bias, weight}
roi_heads.res5.0.conv1.weight
roi_heads.res5.0.conv2.norm.{bias, weight}
roi_heads.res5.0.conv2.weight
roi_heads.res5.0.conv3.norm.{bias, weight}
roi_heads.res5.0.conv3.weight
roi_heads.res5.0.shortcut.norm.{bias, weight}
roi_heads.res5.0.shortcut.weight
roi_heads.res5.1.conv1.norm.{bias, weight}
roi_heads.res5.1.conv1.weight
roi_heads.res5.1.conv2.norm.{bias, weight}
roi_heads.res5.1.conv2.weight
roi_heads.res5.1.conv3.norm.{bias, weight}
roi_heads.res5.1.conv3.weight
roi_heads.res5.2.conv1.norm.{bias, weight}
roi_heads.res5.2.conv1.weight
roi_heads.res5.2.conv2.norm.{bias, weight}
roi_heads.res5.2.conv2.weight
roi_heads.res5.2.conv3.norm.{bias, weight}
roi_heads.res5.2.conv3.weight
[11/28 12:01:12] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  backbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv1.weight
  backbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv2.weight
  backbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.conv3.weight
  backbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.0.shortcut.weight
  backbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv1.weight
  backbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv2.weight
  backbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.1.conv3.weight
  backbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv1.weight
  backbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv2.weight
  backbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res2.2.conv3.weight
  backbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv1.weight
  backbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv2.weight
  backbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.conv3.weight
  backbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.0.shortcut.weight
  backbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv1.weight
  backbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv2.weight
  backbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.1.conv3.weight
  backbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv1.weight
  backbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv2.weight
  backbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.2.conv3.weight
  backbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv1.weight
  backbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv2.weight
  backbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res3.3.conv3.weight
  backbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv1.weight
  backbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv2.weight
  backbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.conv3.weight
  backbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.0.shortcut.weight
  backbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv1.weight
  backbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv2.weight
  backbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.1.conv3.weight
  backbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv1.weight
  backbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv2.weight
  backbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.2.conv3.weight
  backbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv1.weight
  backbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv2.weight
  backbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.3.conv3.weight
  backbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv1.weight
  backbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv2.weight
  backbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.4.conv3.weight
  backbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv1.weight
  backbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv2.weight
  backbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res4.5.conv3.weight
  backbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv1.weight
  backbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv2.weight
  backbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.conv3.weight
  backbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.0.shortcut.weight
  backbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv1.weight
  backbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv2.weight
  backbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.1.conv3.weight
  backbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv1.weight
  backbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv2.weight
  backbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.res5.2.conv3.weight
  backbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}
  backbone.bottom_up.stem.conv1.weight
  backbone.fpn_lateral2.{bias, weight}
  backbone.fpn_lateral3.{bias, weight}
  backbone.fpn_lateral4.{bias, weight}
  backbone.fpn_lateral5.{bias, weight}
  backbone.fpn_output2.{bias, weight}
  backbone.fpn_output3.{bias, weight}
  backbone.fpn_output4.{bias, weight}
  backbone.fpn_output5.{bias, weight}
  roi_heads.box_head.fc1.{bias, weight}
  roi_heads.box_head.fc2.{bias, weight}
[11/28 12:01:35] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:01:35] d2.data.datasets.coco INFO: Loaded 193 images in COCO format from ./dataset/test/labels_test.json
[11/28 12:01:35] d2.data.build INFO: Distribution of instances among all 3 categories:
|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   break    | 220          |    disc    | 193          |   shadow   | 234          |
|            |              |            |              |            |              |
|   total    | 647          |            |              |            |              |
[11/28 12:01:35] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:01:35] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:01:35] d2.data.common INFO: Serializing 193 elements to byte tensors and concatenating them all ...
[11/28 12:01:35] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:01:35] d2.evaluation.evaluator INFO: Start inference on 193 batches
[11/28 12:01:35] d2.evaluation.evaluator INFO: Inference done 11/193. Dataloading: 0.0008 s/iter. Inference: 0.0173 s/iter. Eval: 0.0001 s/iter. Total: 0.0181 s/iter. ETA=0:00:03
[11/28 12:01:39] d2.evaluation.evaluator INFO: Total inference time: 0:00:03.369613 (0.017923 s / iter per device, on 1 devices)
[11/28 12:01:39] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:03 (0.016715 s / iter per device, on 1 devices)
[11/28 12:01:39] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:01:39] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-11-29-55/coco_instances_results.json
[11/28 12:01:39] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:01:39] d2.evaluation.coco_evaluation WARNING: No predictions from the model!
[11/28 12:41:50] detectron2 INFO: Rank of current process: 0. World size: 1
[11/28 12:41:50] detectron2 INFO: Environment info:
----------------------  -----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]
numpy                   1.23.4
detectron2              0.6 @/home/general/detectron2/detectron2
Compiler                GCC 9.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          526.98
CUDA_HOME               /usr
Pillow                  9.3.0
torchvision             0.14.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     4.6.0
----------------------  -----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 12:41:50] detectron2 INFO: Command line arguments: Namespace(config_file='./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[11/28 12:41:50] detectron2 INFO: Contents of args.config_file=./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m../Base-RCNN-FPN.yaml[39m[38;5;186m"[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(210000,[39m[38;5;141m [39m[38;5;141m250000)[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m270000[39m

[11/28 12:41:50] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mval[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mtrain[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBGR[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m672[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m704[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m736[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_fpn_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGeneralizedRCNN[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_FREQ_WEIGHT_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_NUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFastRCNNConvFCHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_FED_LOSS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_SIGMOID_CE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_HEADS_BATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSemSegFPNHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m54[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output/trial-2022-11-28-12-41-50[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBASE_LR_END[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mvalue[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m320[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mNUM_DECAYS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mRESCALE_INTERVAL[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[11/28 12:41:50] detectron2 INFO: Full config saved to ./output/trial-2022-11-28-12-41-50/config.yaml
[11/28 12:41:50] d2.utils.env INFO: Using a generated random seed 50896273
[11/28 12:44:12] detectron2 INFO: Rank of current process: 0. World size: 1
[11/28 12:44:12] detectron2 INFO: Environment info:
----------------------  -----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]
numpy                   1.23.4
detectron2              0.6 @/home/general/detectron2/detectron2
Compiler                GCC 9.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          526.98
CUDA_HOME               /usr
Pillow                  9.3.0
torchvision             0.14.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     4.6.0
----------------------  -----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 12:44:12] detectron2 INFO: Command line arguments: Namespace(config_file='./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[11/28 12:44:12] detectron2 INFO: Contents of args.config_file=./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m../Base-RCNN-FPN.yaml[39m[38;5;186m"[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(210000,[39m[38;5;141m [39m[38;5;141m250000)[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m270000[39m

[11/28 12:44:12] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mval[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mtrain[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBGR[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m672[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m704[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m736[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_fpn_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGeneralizedRCNN[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_FREQ_WEIGHT_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_NUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFastRCNNConvFCHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_FED_LOSS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_SIGMOID_CE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_HEADS_BATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSemSegFPNHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m54[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output/trial-2022-11-28-12-44-12[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBASE_LR_END[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mvalue[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m320[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mNUM_DECAYS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mRESCALE_INTERVAL[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[11/28 12:44:12] detectron2 INFO: Full config saved to ./output/trial-2022-11-28-12-44-12/config.yaml
[11/28 12:44:12] d2.utils.env INFO: Using a generated random seed 12826201
[11/28 12:48:57] detectron2 INFO: Rank of current process: 0. World size: 1
[11/28 12:48:57] detectron2 INFO: Environment info:
----------------------  -----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]
numpy                   1.23.4
detectron2              0.6 @/home/general/detectron2/detectron2
Compiler                GCC 9.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          526.98
CUDA_HOME               /usr
Pillow                  9.3.0
torchvision             0.14.0+cu117 @/home/general/miniconda3/envs/test/lib/python3.10/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     4.6.0
----------------------  -----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 12:48:57] detectron2 INFO: Command line arguments: Namespace(config_file='./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[11/28 12:48:57] detectron2 INFO: Contents of args.config_file=./configs/Detection/faster_rcnn_R_50_FPN_3x.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m../Base-RCNN-FPN.yaml[39m[38;5;186m"[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(210000,[39m[38;5;141m [39m[38;5;141m250000)[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m270000[39m

[11/28 12:48:57] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mval[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mtrain[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBGR[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m672[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m704[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m736[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_fpn_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGeneralizedRCNN[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_FREQ_WEIGHT_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mFED_LOSS_NUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFastRCNNConvFCHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_FED_LOSS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mUSE_SIGMOID_CE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_HEADS_BATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSemSegFPNHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m54[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/MSRA/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output/trial-2022-11-28-12-48-57[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBASE_LR_END[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mvalue[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m320[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mNUM_DECAYS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mRESCALE_INTERVAL[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[11/28 12:48:57] detectron2 INFO: Full config saved to ./output/trial-2022-11-28-12-48-57/config.yaml
[11/28 12:48:57] d2.utils.env INFO: Using a generated random seed 57428040
[11/28 12:48:57] d2.engine.defaults INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=4, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)
    )
  )
)
[11/28 12:48:57] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:48:57] d2.data.datasets.coco INFO: Loaded 565 images in COCO format from ./dataset/train/labels_train.json
[11/28 12:48:57] d2.data.build INFO: Removed 0 images with no usable annotations. 565 images left.
[11/28 12:48:57] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[11/28 12:48:57] d2.data.build INFO: Using training sampler TrainingSampler
[11/28 12:48:57] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:48:57] d2.data.common INFO: Serializing 565 elements to byte tensors and concatenating them all ...
[11/28 12:48:57] d2.data.common INFO: Serialized dataset takes 0.21 MiB
[11/28 12:48:57] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[11/28 12:48:57] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:48:57] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:48:57] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:48:57] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:48:57] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:48:57] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[11/28 12:48:57] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[11/28 12:48:58] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model    | Names in Checkpoint      | Shapes                                          |
|:------------------|:-------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |
[11/28 12:48:58] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
backbone.fpn_lateral2.{bias, weight}
backbone.fpn_lateral3.{bias, weight}
backbone.fpn_lateral4.{bias, weight}
backbone.fpn_lateral5.{bias, weight}
backbone.fpn_output2.{bias, weight}
backbone.fpn_output3.{bias, weight}
backbone.fpn_output4.{bias, weight}
backbone.fpn_output5.{bias, weight}
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_head.fc1.{bias, weight}
roi_heads.box_head.fc2.{bias, weight}
roi_heads.box_predictor.bbox_pred.{bias, weight}
roi_heads.box_predictor.cls_score.{bias, weight}
[11/28 12:48:58] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  fc1000.{bias, weight}
  stem.conv1.bias
[11/28 12:48:58] d2.engine.train_loop INFO: Starting training from iteration 0
[11/28 12:49:11] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:49:11] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:49:11] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:49:11] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:49:11] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:49:11] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:49:11] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:49:11] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:49:12] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0316 s/iter. Eval: 0.0005 s/iter. Total: 0.0327 s/iter. ETA=0:00:05
[11/28 12:49:17] d2.evaluation.evaluator INFO: Inference done 163/184. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0005 s/iter. Total: 0.0330 s/iter. ETA=0:00:00
[11/28 12:49:18] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.915032 (0.033045 s / iter per device, on 1 devices)
[11/28 12:49:18] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031497 s / iter per device, on 1 devices)
[11/28 12:49:18] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:49:18] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:49:18] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:49:18] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:49:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.08 seconds.
[11/28 12:49:18] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:49:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:49:18] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.008 | 0.015  | 0.014  | 0.000 | 0.036 | 0.012 |
[11/28 12:49:18] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.025 |
[11/28 12:49:18] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:49:18] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:49:18] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:49:18] d2.evaluation.testing INFO: copypaste: 0.0083,0.0146,0.0138,0.0000,0.0360,0.0119
[11/28 12:49:29] d2.utils.events INFO:  eta: 0:04:10  iter: 19  total_loss: 2.83  loss_cls: 1.319  loss_box_reg: 0.008615  loss_rpn_cls: 0.7125  loss_rpn_loc: 0.8199  validation_loss: 2.821  time: 0.8286  data_time: 0.0635  lr: 6.0316e-06  max_mem: 10279M
[11/28 12:49:39] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:49:39] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:49:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:49:39] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:49:39] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:49:39] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:49:39] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:49:39] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:49:40] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0320 s/iter. Eval: 0.0005 s/iter. Total: 0.0331 s/iter. ETA=0:00:05
[11/28 12:49:45] d2.evaluation.evaluator INFO: Inference done 161/184. Dataloading: 0.0008 s/iter. Inference: 0.0320 s/iter. Eval: 0.0005 s/iter. Total: 0.0334 s/iter. ETA=0:00:00
[11/28 12:49:46] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.981681 (0.033417 s / iter per device, on 1 devices)
[11/28 12:49:46] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031881 s / iter per device, on 1 devices)
[11/28 12:49:46] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:49:46] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:49:46] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:49:46] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:49:46] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.07 seconds.
[11/28 12:49:46] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:49:46] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:49:46] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.043 | 0.218  | 0.025  | 0.000 | 0.129 | 0.033 |
[11/28 12:49:46] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.074 | shadow     | 0.054 |
[11/28 12:49:46] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:49:46] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:49:46] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:49:46] d2.evaluation.testing INFO: copypaste: 0.0425,0.2185,0.0254,0.0000,0.1294,0.0326
[11/28 12:50:01] d2.utils.events INFO:  eta: 0:03:52  iter: 39  total_loss: 1.966  loss_cls: 0.3956  loss_box_reg: 0.008067  loss_rpn_cls: 0.7062  loss_rpn_loc: 0.8867  validation_loss: 2.465  time: 0.8247  data_time: 0.0528  lr: 1.2275e-05  max_mem: 10279M
[11/28 12:50:08] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:50:08] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:50:08] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:50:08] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:50:08] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:50:08] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:50:08] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:50:08] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:50:08] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0320 s/iter. Eval: 0.0005 s/iter. Total: 0.0331 s/iter. ETA=0:00:05
[11/28 12:50:13] d2.evaluation.evaluator INFO: Inference done 162/184. Dataloading: 0.0009 s/iter. Inference: 0.0317 s/iter. Eval: 0.0005 s/iter. Total: 0.0331 s/iter. ETA=0:00:00
[11/28 12:50:14] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.933308 (0.033147 s / iter per device, on 1 devices)
[11/28 12:50:14] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031550 s / iter per device, on 1 devices)
[11/28 12:50:14] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:50:14] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:50:14] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:50:14] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:50:14] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.09 seconds.
[11/28 12:50:14] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:50:14] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:50:14] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.044 | 0.365  | 0.010  | 0.000 | 0.070 | 0.040 |
[11/28 12:50:14] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.114 | shadow     | 0.019 |
[11/28 12:50:14] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:50:14] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:50:14] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:50:14] d2.evaluation.testing INFO: copypaste: 0.0442,0.3652,0.0097,0.0000,0.0702,0.0405
[11/28 12:50:33] d2.utils.events INFO:  eta: 0:03:37  iter: 59  total_loss: 1.68  loss_cls: 0.08641  loss_box_reg: 0.006751  loss_rpn_cls: 0.693  loss_rpn_loc: 0.8717  validation_loss: 2.109  time: 0.8288  data_time: 0.0551  lr: 1.8519e-05  max_mem: 10279M
[11/28 12:50:36] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:50:36] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:50:36] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:50:36] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:50:36] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:50:36] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:50:36] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:50:36] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:50:37] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0312 s/iter. Eval: 0.0004 s/iter. Total: 0.0324 s/iter. ETA=0:00:05
[11/28 12:50:42] d2.evaluation.evaluator INFO: Inference done 161/184. Dataloading: 0.0009 s/iter. Inference: 0.0320 s/iter. Eval: 0.0005 s/iter. Total: 0.0335 s/iter. ETA=0:00:00
[11/28 12:50:43] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.000823 (0.033524 s / iter per device, on 1 devices)
[11/28 12:50:43] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031926 s / iter per device, on 1 devices)
[11/28 12:50:43] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:50:43] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:50:43] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:50:43] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:50:43] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.06 seconds.
[11/28 12:50:43] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:50:43] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:50:43] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.017 | 0.130  | 0.004  | 0.000 | 0.058 | 0.016 |
[11/28 12:50:43] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.043 | shadow     | 0.008 |
[11/28 12:50:43] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:50:43] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:50:43] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:50:43] d2.evaluation.testing INFO: copypaste: 0.0169,0.1299,0.0035,0.0000,0.0582,0.0161
[11/28 12:51:05] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:51:05] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:51:05] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:51:05] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:51:05] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:51:05] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:51:05] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:51:05] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:51:06] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0334 s/iter. Eval: 0.0006 s/iter. Total: 0.0347 s/iter. ETA=0:00:06
[11/28 12:51:11] d2.evaluation.evaluator INFO: Inference done 157/184. Dataloading: 0.0009 s/iter. Inference: 0.0318 s/iter. Eval: 0.0015 s/iter. Total: 0.0343 s/iter. ETA=0:00:00
[11/28 12:51:12] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.126455 (0.034226 s / iter per device, on 1 devices)
[11/28 12:51:12] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031761 s / iter per device, on 1 devices)
[11/28 12:51:12] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:51:12] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:51:12] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:51:12] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:51:12] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.07 seconds.
[11/28 12:51:12] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:51:12] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:51:12] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.006 | 0.047  | 0.000  | 0.000 | 0.022 | 0.006 |
[11/28 12:51:12] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.016 | shadow     | 0.003 |
[11/28 12:51:12] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:51:12] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:51:12] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:51:12] d2.evaluation.testing INFO: copypaste: 0.0064,0.0469,0.0000,0.0000,0.0215,0.0064
[11/28 12:51:20] d2.utils.events INFO:  eta: 0:03:21  iter: 79  total_loss: 1.627  loss_cls: 0.06156  loss_box_reg: 0.005804  loss_rpn_cls: 0.6719  loss_rpn_loc: 0.8889  validation_loss: 1.838  time: 0.8348  data_time: 0.0561  lr: 2.4763e-05  max_mem: 10279M
[11/28 12:51:34] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:51:34] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:51:34] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:51:34] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:51:34] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:51:34] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:51:34] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:51:34] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:51:35] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0333 s/iter. Eval: 0.0005 s/iter. Total: 0.0344 s/iter. ETA=0:00:05
[11/28 12:51:40] d2.evaluation.evaluator INFO: Inference done 160/184. Dataloading: 0.0009 s/iter. Inference: 0.0322 s/iter. Eval: 0.0005 s/iter. Total: 0.0337 s/iter. ETA=0:00:00
[11/28 12:51:41] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.036250 (0.033722 s / iter per device, on 1 devices)
[11/28 12:51:41] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032124 s / iter per device, on 1 devices)
[11/28 12:51:41] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:51:41] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:51:41] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:51:41] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:51:41] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.19 seconds.
[11/28 12:51:41] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:51:41] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:51:41] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.001 | 0.008  | 0.000  | 0.000 | 0.001 | 0.001 |
[11/28 12:51:41] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.002 | shadow     | 0.001 |
[11/28 12:51:41] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:51:41] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:51:41] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:51:41] d2.evaluation.testing INFO: copypaste: 0.0011,0.0083,0.0000,0.0000,0.0007,0.0014
[11/28 12:51:53] d2.utils.events INFO:  eta: 0:03:05  iter: 99  total_loss: 1.517  loss_cls: 0.08006  loss_box_reg: 0.00719  loss_rpn_cls: 0.6421  loss_rpn_loc: 0.7788  validation_loss: 1.817  time: 0.8413  data_time: 0.0557  lr: 3.1007e-05  max_mem: 10405M
[11/28 12:52:04] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:52:04] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:52:04] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:52:04] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:52:04] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:52:04] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:52:04] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:52:04] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:52:04] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0321 s/iter. Eval: 0.0005 s/iter. Total: 0.0332 s/iter. ETA=0:00:05
[11/28 12:52:09] d2.evaluation.evaluator INFO: Inference done 158/184. Dataloading: 0.0009 s/iter. Inference: 0.0326 s/iter. Eval: 0.0005 s/iter. Total: 0.0341 s/iter. ETA=0:00:00
[11/28 12:52:10] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.101692 (0.034088 s / iter per device, on 1 devices)
[11/28 12:52:10] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032468 s / iter per device, on 1 devices)
[11/28 12:52:10] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:52:10] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:52:10] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:52:10] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:52:10] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.07 seconds.
[11/28 12:52:10] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:52:10] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:52:10] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.002 | 0.012  | 0.000  | 0.000 | 0.000 | 0.004 |
[11/28 12:52:10] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.001 | shadow     | 0.003 |
[11/28 12:52:10] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:52:10] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:52:10] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:52:11] d2.evaluation.testing INFO: copypaste: 0.0016,0.0115,0.0000,0.0000,0.0001,0.0037
[11/28 12:52:26] d2.utils.events INFO:  eta: 0:02:49  iter: 119  total_loss: 1.331  loss_cls: 0.09578  loss_box_reg: 0.006374  loss_rpn_cls: 0.6086  loss_rpn_loc: 0.6189  validation_loss: 1.796  time: 0.8502  data_time: 0.0538  lr: 3.725e-05  max_mem: 10405M
[11/28 12:52:34] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:52:34] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:52:34] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:52:34] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:52:34] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:52:34] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:52:34] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:52:34] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:52:34] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0334 s/iter. Eval: 0.0005 s/iter. Total: 0.0346 s/iter. ETA=0:00:05
[11/28 12:52:39] d2.evaluation.evaluator INFO: Inference done 156/184. Dataloading: 0.0009 s/iter. Inference: 0.0331 s/iter. Eval: 0.0005 s/iter. Total: 0.0346 s/iter. ETA=0:00:00
[11/28 12:52:40] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.174308 (0.034493 s / iter per device, on 1 devices)
[11/28 12:52:40] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032894 s / iter per device, on 1 devices)
[11/28 12:52:40] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:52:40] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:52:40] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:52:40] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:52:40] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.07 seconds.
[11/28 12:52:40] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:52:40] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.02 seconds.
[11/28 12:52:40] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.001 | 0.009  | 0.000  | 0.000 | 0.000 | 0.004 |
[11/28 12:52:40] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.001 | shadow     | 0.003 |
[11/28 12:52:41] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:52:41] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:52:41] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:52:41] d2.evaluation.testing INFO: copypaste: 0.0014,0.0093,0.0000,0.0000,0.0000,0.0039
[11/28 12:53:00] d2.utils.events INFO:  eta: 0:02:34  iter: 139  total_loss: 1.29  loss_cls: 0.08011  loss_box_reg: 0.0096  loss_rpn_cls: 0.5484  loss_rpn_loc: 0.6437  validation_loss: 1.767  time: 0.8635  data_time: 0.0584  lr: 4.3494e-05  max_mem: 10405M
[11/28 12:53:04] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:53:04] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:53:04] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:53:04] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:53:04] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:53:04] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:53:04] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:53:04] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:53:05] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0338 s/iter. Eval: 0.0004 s/iter. Total: 0.0348 s/iter. ETA=0:00:06
[11/28 12:53:10] d2.evaluation.evaluator INFO: Inference done 155/184. Dataloading: 0.0009 s/iter. Inference: 0.0334 s/iter. Eval: 0.0004 s/iter. Total: 0.0348 s/iter. ETA=0:00:01
[11/28 12:53:11] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.213932 (0.034715 s / iter per device, on 1 devices)
[11/28 12:53:11] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.033208 s / iter per device, on 1 devices)
[11/28 12:53:11] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:53:11] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:53:11] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:53:11] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:53:11] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.03 seconds.
[11/28 12:53:11] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:53:11] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:53:11] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.001 | 0.009  | 0.000  | 0.000 | 0.000 | 0.003 |
[11/28 12:53:11] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.004 |
[11/28 12:53:11] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:53:11] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:53:11] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:53:11] d2.evaluation.testing INFO: copypaste: 0.0014,0.0085,0.0000,0.0000,0.0000,0.0031
[11/28 12:53:38] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:53:38] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:53:38] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:53:38] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:53:38] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:53:38] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:53:38] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:53:38] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:53:39] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0008 s/iter. Inference: 0.0336 s/iter. Eval: 0.0004 s/iter. Total: 0.0348 s/iter. ETA=0:00:06
[11/28 12:53:44] d2.evaluation.evaluator INFO: Inference done 155/184. Dataloading: 0.0009 s/iter. Inference: 0.0334 s/iter. Eval: 0.0003 s/iter. Total: 0.0348 s/iter. ETA=0:00:01
[11/28 12:53:45] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.209405 (0.034689 s / iter per device, on 1 devices)
[11/28 12:53:45] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.033197 s / iter per device, on 1 devices)
[11/28 12:53:45] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:53:45] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:53:45] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:53:45] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:53:45] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:53:45] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:53:45] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:53:45] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.001 | 0.002  | 0.002  | 0.000 | 0.000 | 0.002 |
[11/28 12:53:45] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.004 |
[11/28 12:53:45] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:53:45] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:53:45] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:53:45] d2.evaluation.testing INFO: copypaste: 0.0012,0.0020,0.0020,0.0000,0.0000,0.0024
[11/28 12:53:53] d2.utils.events INFO:  eta: 0:02:18  iter: 159  total_loss: 1.051  loss_cls: 0.1191  loss_box_reg: 0.03731  loss_rpn_cls: 0.4535  loss_rpn_loc: 0.431  validation_loss: 1.719  time: 0.8942  data_time: 0.0532  lr: 4.9738e-05  max_mem: 10405M
[11/28 12:54:12] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:54:12] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:54:12] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:54:12] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:54:12] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:54:12] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:54:12] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:54:12] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:54:13] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0338 s/iter. Eval: 0.0003 s/iter. Total: 0.0347 s/iter. ETA=0:00:06
[11/28 12:54:18] d2.evaluation.evaluator INFO: Inference done 154/184. Dataloading: 0.0009 s/iter. Inference: 0.0339 s/iter. Eval: 0.0003 s/iter. Total: 0.0352 s/iter. ETA=0:00:01
[11/28 12:54:19] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.265345 (0.035002 s / iter per device, on 1 devices)
[11/28 12:54:19] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:06 (0.033533 s / iter per device, on 1 devices)
[11/28 12:54:19] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:54:19] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:54:19] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:54:19] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:54:19] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:54:19] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:54:19] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:54:19] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 12:54:19] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:54:19] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:54:19] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:54:19] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:54:19] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 12:54:32] d2.utils.events INFO:  eta: 0:02:03  iter: 179  total_loss: 1.001  loss_cls: 0.1084  loss_box_reg: 0.03552  loss_rpn_cls: 0.3887  loss_rpn_loc: 0.4796  validation_loss: 1.701  time: 0.9252  data_time: 0.0578  lr: 5.5982e-05  max_mem: 10405M
[11/28 12:54:46] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:54:46] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:54:46] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:54:46] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:54:46] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:54:46] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:54:46] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:54:46] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:54:47] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0324 s/iter. Eval: 0.0002 s/iter. Total: 0.0334 s/iter. ETA=0:00:05
[11/28 12:54:52] d2.evaluation.evaluator INFO: Inference done 162/184. Dataloading: 0.0009 s/iter. Inference: 0.0319 s/iter. Eval: 0.0002 s/iter. Total: 0.0331 s/iter. ETA=0:00:00
[11/28 12:54:53] d2.evaluation.evaluator INFO: Total inference time: 0:00:05.942297 (0.033197 s / iter per device, on 1 devices)
[11/28 12:54:53] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.031816 s / iter per device, on 1 devices)
[11/28 12:54:53] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:54:53] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:54:53] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:54:53] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:54:53] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:54:53] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:54:53] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:54:53] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 12:54:53] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:54:53] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:54:53] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:54:53] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:54:53] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 12:55:11] d2.utils.events INFO:  eta: 0:01:46  iter: 199  total_loss: 0.8263  loss_cls: 0.1028  loss_box_reg: 0.03736  loss_rpn_cls: 0.2975  loss_rpn_loc: 0.3816  validation_loss: 1.65  time: 0.9496  data_time: 0.0547  lr: 6.2225e-05  max_mem: 10405M
[11/28 12:55:20] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:55:20] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:55:20] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:55:20] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:55:20] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:55:20] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:55:20] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:55:20] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:55:21] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0331 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:05
[11/28 12:55:26] d2.evaluation.evaluator INFO: Inference done 159/184. Dataloading: 0.0009 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0338 s/iter. ETA=0:00:00
[11/28 12:55:27] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.087051 (0.034006 s / iter per device, on 1 devices)
[11/28 12:55:27] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.032729 s / iter per device, on 1 devices)
[11/28 12:55:27] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:55:27] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:55:27] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:55:27] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:55:27] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:55:27] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:55:27] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:55:27] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 12:55:27] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:55:27] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:55:27] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:55:27] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:55:27] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 12:55:50] d2.utils.events INFO:  eta: 0:01:30  iter: 219  total_loss: 0.6973  loss_cls: 0.101  loss_box_reg: 0.04534  loss_rpn_cls: 0.2349  loss_rpn_loc: 0.3126  validation_loss: 1.599  time: 0.9669  data_time: 0.0554  lr: 6.8469e-05  max_mem: 10405M
[11/28 12:55:54] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:55:54] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:55:54] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:55:54] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:55:54] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:55:54] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:55:54] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:55:54] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:55:55] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0338 s/iter. Eval: 0.0004 s/iter. Total: 0.0348 s/iter. ETA=0:00:06
[11/28 12:56:00] d2.evaluation.evaluator INFO: Inference done 156/184. Dataloading: 0.0009 s/iter. Inference: 0.0333 s/iter. Eval: 0.0004 s/iter. Total: 0.0346 s/iter. ETA=0:00:00
[11/28 12:56:01] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.203078 (0.034654 s / iter per device, on 1 devices)
[11/28 12:56:01] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.033129 s / iter per device, on 1 devices)
[11/28 12:56:01] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:56:01] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:56:01] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:56:01] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:56:01] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:56:01] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:56:01] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:56:01] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.001  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 12:56:01] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:56:01] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:56:01] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:56:01] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:56:01] d2.evaluation.testing INFO: copypaste: 0.0001,0.0005,0.0000,0.0000,0.0002,0.0000
[11/28 12:56:29] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:56:29] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:56:29] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:56:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:56:29] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:56:29] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:56:29] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:56:29] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:56:29] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0339 s/iter. Eval: 0.0004 s/iter. Total: 0.0350 s/iter. ETA=0:00:06
[11/28 12:56:34] d2.evaluation.evaluator INFO: Inference done 152/184. Dataloading: 0.0009 s/iter. Inference: 0.0343 s/iter. Eval: 0.0004 s/iter. Total: 0.0356 s/iter. ETA=0:00:01
[11/28 12:56:35] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.388535 (0.035690 s / iter per device, on 1 devices)
[11/28 12:56:35] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:06 (0.034164 s / iter per device, on 1 devices)
[11/28 12:56:35] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:56:35] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:56:35] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:56:35] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:56:35] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:56:35] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:56:35] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:56:35] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 12:56:35] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:56:35] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:56:35] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:56:35] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:56:35] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 12:56:45] d2.utils.events INFO:  eta: 0:01:13  iter: 239  total_loss: 0.6898  loss_cls: 0.08978  loss_box_reg: 0.0394  loss_rpn_cls: 0.1887  loss_rpn_loc: 0.3467  validation_loss: 1.384  time: 0.9807  data_time: 0.0552  lr: 7.4713e-05  max_mem: 10405M
[11/28 12:57:03] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:57:03] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:57:03] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:57:03] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:57:03] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:57:03] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:57:03] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:57:03] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:57:04] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0008 s/iter. Inference: 0.0338 s/iter. Eval: 0.0004 s/iter. Total: 0.0349 s/iter. ETA=0:00:06
[11/28 12:57:09] d2.evaluation.evaluator INFO: Inference done 146/184. Dataloading: 0.0009 s/iter. Inference: 0.0356 s/iter. Eval: 0.0004 s/iter. Total: 0.0370 s/iter. ETA=0:00:01
[11/28 12:57:10] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.543694 (0.036557 s / iter per device, on 1 devices)
[11/28 12:57:10] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:06 (0.034994 s / iter per device, on 1 devices)
[11/28 12:57:10] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:57:10] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:57:10] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:57:10] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:57:10] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:57:10] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:57:10] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:57:10] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 12:57:10] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:57:10] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:57:10] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:57:10] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:57:10] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 12:57:24] d2.utils.events INFO:  eta: 0:00:56  iter: 259  total_loss: 0.5944  loss_cls: 0.09212  loss_box_reg: 0.04241  loss_rpn_cls: 0.1896  loss_rpn_loc: 0.256  validation_loss: 1.296  time: 0.9931  data_time: 0.0543  lr: 8.0957e-05  max_mem: 10405M
[11/28 12:57:37] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:57:37] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:57:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:57:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:57:37] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:57:38] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:57:38] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:57:38] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:57:38] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0008 s/iter. Inference: 0.0340 s/iter. Eval: 0.0003 s/iter. Total: 0.0351 s/iter. ETA=0:00:06
[11/28 12:57:43] d2.evaluation.evaluator INFO: Inference done 151/184. Dataloading: 0.0009 s/iter. Inference: 0.0344 s/iter. Eval: 0.0003 s/iter. Total: 0.0357 s/iter. ETA=0:00:01
[11/28 12:57:44] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.367769 (0.035574 s / iter per device, on 1 devices)
[11/28 12:57:44] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:06 (0.034105 s / iter per device, on 1 devices)
[11/28 12:57:44] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:57:44] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:57:44] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:57:44] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:57:44] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:57:44] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:57:44] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:57:44] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.001 | 0.002  | 0.001  | 0.000 | 0.004 | 0.000 |
[11/28 12:57:44] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.003 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:57:44] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:57:44] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:57:44] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:57:44] d2.evaluation.testing INFO: copypaste: 0.0010,0.0022,0.0014,0.0000,0.0037,0.0000
[11/28 12:58:02] d2.utils.events INFO:  eta: 0:00:40  iter: 279  total_loss: 0.6638  loss_cls: 0.09312  loss_box_reg: 0.04521  loss_rpn_cls: 0.1872  loss_rpn_loc: 0.3345  validation_loss: 1.209  time: 1.0026  data_time: 0.0540  lr: 8.72e-05  max_mem: 10405M
[11/28 12:58:11] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:58:11] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:58:11] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:58:11] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:58:11] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:58:11] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:58:11] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:58:11] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:58:12] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0343 s/iter. Eval: 0.0003 s/iter. Total: 0.0353 s/iter. ETA=0:00:06
[11/28 12:58:17] d2.evaluation.evaluator INFO: Inference done 155/184. Dataloading: 0.0009 s/iter. Inference: 0.0335 s/iter. Eval: 0.0003 s/iter. Total: 0.0348 s/iter. ETA=0:00:01
[11/28 12:58:18] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.220683 (0.034752 s / iter per device, on 1 devices)
[11/28 12:58:18] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:05 (0.033265 s / iter per device, on 1 devices)
[11/28 12:58:18] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:58:18] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:58:18] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:58:18] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:58:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:58:18] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:58:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:58:18] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.002 | 0.010  | 0.000  | 0.000 | 0.008 | 0.000 |
[11/28 12:58:18] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.005 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:58:18] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:58:18] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:58:18] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:58:18] d2.evaluation.testing INFO: copypaste: 0.0016,0.0097,0.0000,0.0000,0.0082,0.0000
[11/28 12:58:41] d2.utils.events INFO:  eta: 0:00:21  iter: 299  total_loss: 0.5874  loss_cls: 0.08463  loss_box_reg: 0.03836  loss_rpn_cls: 0.1726  loss_rpn_loc: 0.2899  validation_loss: 1.168  time: 1.0107  data_time: 0.0545  lr: 9.3444e-05  max_mem: 10405M
[11/28 12:58:46] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:58:46] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:58:46] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:58:46] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:58:46] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:58:46] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:58:46] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:58:46] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:58:46] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0007 s/iter. Inference: 0.0333 s/iter. Eval: 0.0003 s/iter. Total: 0.0342 s/iter. ETA=0:00:05
[11/28 12:58:51] d2.evaluation.evaluator INFO: Inference done 153/184. Dataloading: 0.0009 s/iter. Inference: 0.0339 s/iter. Eval: 0.0003 s/iter. Total: 0.0352 s/iter. ETA=0:00:01
[11/28 12:58:52] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.299940 (0.035195 s / iter per device, on 1 devices)
[11/28 12:58:52] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:06 (0.033703 s / iter per device, on 1 devices)
[11/28 12:58:52] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:58:52] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:58:52] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:58:52] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:58:52] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:58:52] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:58:52] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:58:52] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |
[11/28 12:58:52] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.000 | disc       | 0.000 | shadow     | 0.000 |
[11/28 12:58:52] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:58:52] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:58:52] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:58:52] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000
[11/28 12:59:19] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/trial-2022-11-28-12-48-57/model_final.pth
[11/28 12:59:29] d2.utils.events INFO:  eta: 0:00:00  iter: 319  total_loss: 0.606  loss_cls: 0.08915  loss_box_reg: 0.04266  loss_rpn_cls: 0.1699  loss_rpn_loc: 0.2918  validation_loss: 1.094  time: 1.0181  data_time: 0.0594  lr: 9.9688e-05  max_mem: 10405M
[11/28 12:59:29] d2.engine.hooks INFO: Overall training speed: 318 iterations in 0:05:23 (1.0181 s / it)
[11/28 12:59:29] d2.engine.hooks INFO: Total training time: 0:10:28 (0:05:05 on hooks)
[11/28 12:59:29] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[11/28 12:59:29] d2.data.datasets.coco INFO: Loaded 184 images in COCO format from ./dataset/val/labels_val.json
[11/28 12:59:29] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[11/28 12:59:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[11/28 12:59:29] d2.data.common INFO: Serializing 184 elements to byte tensors and concatenating them all ...
[11/28 12:59:29] d2.data.common INFO: Serialized dataset takes 0.07 MiB
[11/28 12:59:29] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[11/28 12:59:29] d2.evaluation.evaluator INFO: Start inference on 184 batches
[11/28 12:59:29] d2.evaluation.evaluator INFO: Inference done 11/184. Dataloading: 0.0006 s/iter. Inference: 0.0359 s/iter. Eval: 0.0003 s/iter. Total: 0.0369 s/iter. ETA=0:00:06
[11/28 12:59:34] d2.evaluation.evaluator INFO: Inference done 148/184. Dataloading: 0.0009 s/iter. Inference: 0.0353 s/iter. Eval: 0.0003 s/iter. Total: 0.0366 s/iter. ETA=0:00:01
[11/28 12:59:36] d2.evaluation.evaluator INFO: Total inference time: 0:00:06.543581 (0.036556 s / iter per device, on 1 devices)
[11/28 12:59:36] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:06 (0.035094 s / iter per device, on 1 devices)
[11/28 12:59:36] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[11/28 12:59:36] d2.evaluation.coco_evaluation INFO: Saving results to ./output/trial-2022-11-28-12-48-57/inference/coco_instances_results.json
[11/28 12:59:36] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[11/28 12:59:36] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[11/28 12:59:36] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.02 seconds.
[11/28 12:59:36] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[11/28 12:59:36] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[11/28 12:59:36] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.018 | 0.169  | 0.000  | 0.000 | 0.005 | 0.028 |
[11/28 12:59:36] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|:-----------|:------|
| break      | 0.006 | disc       | 0.033 | shadow     | 0.017 |
[11/28 12:59:36] d2.engine.defaults INFO: Evaluation results for val in csv format:
[11/28 12:59:36] d2.evaluation.testing INFO: copypaste: Task: bbox
[11/28 12:59:36] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[11/28 12:59:36] d2.evaluation.testing INFO: copypaste: 0.0184,0.1687,0.0000,0.0000,0.0049,0.0275
